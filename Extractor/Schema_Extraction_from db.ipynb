{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a609dca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SELECT DISTINCT INSTITUTION_NAME FROM `finance-analytics-291816.BI_Sales_Chatbot.layer4_FT_VW_FACT_DAILY_SALES_DATA_ANALYSIS` \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Sarthak\\Prod_Deployment\\v14-Sreyas - Copy\\.venv\\Lib\\site-packages\\google\\cloud\\bigquery\\table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INSTITUTION_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Army-MH-Bathinda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ankura Hospitals Khammam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MANIPAL HOSPITALS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acharya Vinoba Bhave Hosp-Wardha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tata 1mg Health and Solutions Pvt l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      INSTITUTION_NAME\n",
       "0                     Army-MH-Bathinda\n",
       "1             Ankura Hospitals Khammam\n",
       "2                    MANIPAL HOSPITALS\n",
       "3     Acharya Vinoba Bhave Hosp-Wardha\n",
       "4  Tata 1mg Health and Solutions Pvt l"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "import pandas\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"finance-analytics-291816-4e2d4b15e5d5 5.json\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "#Test\n",
    "sql=\"\"\"\n",
    "SELECT DISTINCT INSTITUTION_NAME FROM `finance-analytics-291816.BI_Sales_Chatbot.layer4_FT_VW_FACT_DAILY_SALES_DATA_ANALYSIS` \n",
    "\"\"\"\n",
    "query_job = client.query(\n",
    "    sql,\n",
    "    job_id_prefix='test_sql_query'\n",
    ")\n",
    "print(sql)\n",
    "df_v1 = query_job.to_dataframe()\n",
    "# df.to_csv(r'C://Users//P90020194//Desktop//Projects//Extracted_Tables_2//Customer_Table')\n",
    "\n",
    "# print(df)\n",
    "df_v1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de342c1a",
   "metadata": {},
   "source": [
    "# Step 3: Get Table Schema (Columns, Types, Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "725d6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from typing import Dict, List, Optional, Any\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "92abed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your specific table details\n",
    "PROJECT_ID = \"finance-analytics-291816\"\n",
    "DATASET_ID = \"BI_Sales_Chatbot\"\n",
    "TABLE_ID = \"layer4_FT_VW_FACT_DAILY_SALES_DATA_ANALYSIS\"\n",
    "TABLE_ID=\"Product\"\n",
    "TABLE_ID=\"layer4_FT_VW_FACT_SECONDARY_SALES_DATA\"\n",
    "TABLE_ID=\"VW_FT_FACT_EXTERNAL_SALES\"\n",
    "TABLE_ID=\"CUSTOMER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "13340650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Target Table: finance-analytics-291816.BI_Sales_Chatbot.CUSTOMER\n",
      "‚úÖ Schema extracted for BI_Sales_Chatbot.CUSTOMER\n",
      "üìä Found 15 columns\n",
      "\n",
      "üìã Table Schema Preview:\n",
      "             column_name data_type      mode\n",
      "0          CUSTOMER_CODE    STRING  NULLABLE\n",
      "1          CUSTOMER_NAME    STRING  NULLABLE\n",
      "2           COUNTRY_CODE    STRING  NULLABLE\n",
      "3           COUNTRY_NAME    STRING  NULLABLE\n",
      "4             STATE_CODE    STRING  NULLABLE\n",
      "5             STATE_NAME    STRING  NULLABLE\n",
      "6             STATE_ZONE    STRING  NULLABLE\n",
      "7         CUSTOMER_GROUP    STRING  NULLABLE\n",
      "8  CUSTOMER_BUYING_GROUP    STRING  NULLABLE\n",
      "9      PUBLIC_OR_PRIVATE    STRING  NULLABLE\n",
      "\n",
      "... and 5 more columns\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_table_schema(client: bigquery.Client, dataset_id: str, table_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract table schema including column names, types, and constraints\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get table reference\n",
    "        table_ref = client.dataset(dataset_id).table(table_id)\n",
    "        table = client.get_table(table_ref)\n",
    "        \n",
    "        schema_data = []\n",
    "        for field in table.schema:\n",
    "            schema_data.append({\n",
    "                'column_name': field.name,\n",
    "                'data_type': field.field_type,\n",
    "                'mode': field.mode,  # NULLABLE, REQUIRED, REPEATED\n",
    "                'is_nullable': field.mode == 'NULLABLE',\n",
    "                'description': field.description or '',\n",
    "                'max_length': getattr(field, 'max_length', None)\n",
    "            })\n",
    "        \n",
    "        schema_df = pd.DataFrame(schema_data)\n",
    "        print(f\"‚úÖ Schema extracted for {dataset_id}.{table_id}\")\n",
    "        print(f\"üìä Found {len(schema_df)} columns\")\n",
    "        return schema_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting table schema: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "print(f\"üéØ Target Table: {PROJECT_ID}.{DATASET_ID}.{TABLE_ID}\")\n",
    "\n",
    "# Test the connection and get basic schema\n",
    "schema_df = get_table_schema(client, DATASET_ID, TABLE_ID)\n",
    "print(\"\\nüìã Table Schema Preview:\")\n",
    "if not schema_df.empty:\n",
    "    print(schema_df[['column_name', 'data_type', 'mode']].head(10))\n",
    "    print(f\"\\n... and {max(0, len(schema_df) - 10)} more columns\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Could not retrieve schema. Check your credentials and table access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bce6bfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema_df[(schema_df['data_type'] == \"FLOAT\") | (schema_df['data_type'] == \"NUMERIC\")]['column_name'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3d469",
   "metadata": {},
   "source": [
    "# Step 4: Identify Primary Keys (Based on Constraints and Patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "12bdcdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_primary_keys(client: bigquery.Client, dataset_id: str, table_id: str, \n",
    "                         schema_df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Identify potential primary keys based on:\n",
    "    1. Column names (contains 'id', 'key', 'pk')\n",
    "    2. Uniqueness check\n",
    "    3. Non-null constraint\n",
    "    \"\"\"\n",
    "    potential_pks = []\n",
    "    \n",
    "    # Check for typical PK naming patterns\n",
    "    pk_patterns = ['id', 'key', 'pk', '_id']\n",
    "    for _, row in schema_df.iterrows():\n",
    "        col_name = row['column_name'].lower()\n",
    "        if any(pattern in col_name for pattern in pk_patterns):\n",
    "            if not row['is_nullable']:  # Should not be nullable\n",
    "                potential_pks.append(row['column_name'])\n",
    "    \n",
    "    # Verify uniqueness for potential PKs\n",
    "    verified_pks = []\n",
    "    for pk_col in potential_pks:\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_rows,\n",
    "                COUNT(DISTINCT {pk_col}) as unique_values\n",
    "            FROM `{client.project}.{dataset_id}.{table_id}`\n",
    "            \"\"\"\n",
    "            result = client.query(query).result()\n",
    "            for row in result:\n",
    "                if row.total_rows == row.unique_values and row.total_rows > 0:\n",
    "                    verified_pks.append(pk_col)\n",
    "                    print(f\"‚úÖ Verified PK: {pk_col} ({row.total_rows} unique values)\")\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  {pk_col} is not unique ({row.unique_values}/{row.total_rows})\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error checking {pk_col}: {e}\")\n",
    "    \n",
    "    return verified_pks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b517a0fc",
   "metadata": {},
   "source": [
    "# Step 5: Identify Categorical/ Numerical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "11928ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_categorical_columns(schema_df: pd.DataFrame, \n",
    "                               max_distinct_threshold: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Identify potential categorical columns based on data type\n",
    "    \"\"\"\n",
    "    categorical_cols = []\n",
    "    \n",
    "    for _, row in schema_df.iterrows():\n",
    "        col_name = row['column_name']\n",
    "        data_type = row['data_type']\n",
    "        \n",
    "        # String columns are potential categoricals\n",
    "        if data_type in ['STRING', 'BYTES']:\n",
    "            categorical_cols.append(col_name)\n",
    "        # Integer columns with specific naming patterns might be categorical\n",
    "        elif data_type in ['INTEGER', 'INT64'] and any(\n",
    "            pattern in col_name.lower() \n",
    "            for pattern in ['type', 'status', 'category', 'grade', 'level', 'rating']\n",
    "        ):\n",
    "            categorical_cols.append(col_name)\n",
    "    \n",
    "    print(f\"üìä Identified {len(categorical_cols)} potential categorical columns\")\n",
    "    return categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a4f22506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_numerical_columns(schema_df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Identify numerical columns for statistical analysis\n",
    "    \"\"\"\n",
    "    numerical_cols = []\n",
    "    \n",
    "    for _, row in schema_df.iterrows():\n",
    "        col_name = row['column_name']\n",
    "        data_type = row['data_type']\n",
    "        \n",
    "        # All numeric data types\n",
    "        if data_type in ['INTEGER', 'INT64', 'FLOAT', 'FLOAT64', 'NUMERIC', 'BIGNUMERIC']:\n",
    "            numerical_cols.append(col_name)\n",
    "    \n",
    "    print(f\"üìä Identified {len(numerical_cols)} numerical columns\")\n",
    "    return numerical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad56fc6",
   "metadata": {},
   "source": [
    "# Step 6: Extract Distinct Values for Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9d8dd3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_distinct_values(client: bigquery.Client, dataset_id: str, table_id: str,\n",
    "                                  categorical_cols: List[str], \n",
    "                                  max_distinct_per_column: int = 100000) -> Dict[str, List]:\n",
    "    \"\"\"\n",
    "    Extract distinct values for each categorical column\n",
    "    \"\"\"\n",
    "    distinct_values = {}\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                {col},\n",
    "                COUNT(*) as frequency\n",
    "            FROM `{client.project}.{dataset_id}.{table_id}`\n",
    "            WHERE {col} IS NOT NULL\n",
    "            GROUP BY {col}\n",
    "            ORDER BY frequency DESC\n",
    "            LIMIT {max_distinct_per_column}\n",
    "            \"\"\"\n",
    "            \n",
    "            result = client.query(query).result()\n",
    "            values_list = []\n",
    "            total_frequency = 0\n",
    "            \n",
    "            for row in result:\n",
    "                values_list.append({\n",
    "                    'value': row[0],\n",
    "                    'frequency': row[1]\n",
    "                })\n",
    "                total_frequency += row[1]\n",
    "            \n",
    "            distinct_values[col] = {\n",
    "                'values': [item['value'] for item in values_list],\n",
    "                'value_counts': values_list,\n",
    "                'total_distinct': len(values_list),\n",
    "                'total_frequency': total_frequency\n",
    "            }\n",
    "            \n",
    "            print(f\"‚úÖ {col}: {len(values_list)} distinct values extracted\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting values for {col}: {e}\")\n",
    "            distinct_values[col] = {'values': [], 'error': str(e)}\n",
    "    \n",
    "    return distinct_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df5466d",
   "metadata": {},
   "source": [
    "# Step 6.1- Get Numerial olcumn Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b37d5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_column_statistics(client: bigquery.Client, dataset_id: str, table_id: str,\n",
    "                                  numerical_cols: List[str]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Extract statistical summary for numerical columns\n",
    "    \"\"\"\n",
    "    numerical_stats = {}\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        try:\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_rows,\n",
    "                COUNT({col}) as non_null_rows,\n",
    "                COUNT(*) - COUNT({col}) as null_rows,\n",
    "                MIN({col}) as min_value,\n",
    "                MAX({col}) as max_value,\n",
    "                AVG({col}) as mean_value,\n",
    "                STDDEV({col}) as std_dev,\n",
    "                -- Percentiles for distribution analysis\n",
    "                APPROX_QUANTILES({col}, 100)[OFFSET(25)] as percentile_25,\n",
    "                APPROX_QUANTILES({col}, 100)[OFFSET(50)] as median,\n",
    "                APPROX_QUANTILES({col}, 100)[OFFSET(75)] as percentile_75,\n",
    "                -- Count distinct values to check if it's actually categorical\n",
    "                COUNT(DISTINCT {col}) as distinct_count\n",
    "            FROM `{client.project}.{dataset_id}.{table_id}`\n",
    "            \"\"\"\n",
    "            \n",
    "            result = client.query(query).result()\n",
    "            \n",
    "            for row in result:\n",
    "                # Check if this \"numerical\" column is actually categorical (low distinct count)\n",
    "                is_actually_categorical = row.distinct_count <= 50 and row.non_null_rows > 0\n",
    "                \n",
    "                numerical_stats[col] = {\n",
    "                    'total_rows': row.total_rows,\n",
    "                    'non_null_rows': row.non_null_rows,\n",
    "                    'null_rows': row.null_rows,\n",
    "                    'null_percentage': (row.null_rows / row.total_rows * 100) if row.total_rows > 0 else 0,\n",
    "                    'distinct_count': row.distinct_count,\n",
    "                    'min_value': float(row.min_value) if row.min_value is not None else None,\n",
    "                    'max_value': float(row.max_value) if row.max_value is not None else None,\n",
    "                    'mean_value': float(row.mean_value) if row.mean_value is not None else None,\n",
    "                    'std_dev': float(row.std_dev) if row.std_dev is not None else None,\n",
    "                    'median': float(row.median) if row.median is not None else None,\n",
    "                    'percentile_25': float(row.percentile_25) if row.percentile_25 is not None else None,\n",
    "                    'percentile_75': float(row.percentile_75) if row.percentile_75 is not None else None,\n",
    "                    'is_actually_categorical': is_actually_categorical\n",
    "                }\n",
    "                \n",
    "                # Calculate range and IQR\n",
    "                if row.min_value is not None and row.max_value is not None:\n",
    "                    numerical_stats[col]['range'] = float(row.max_value) - float(row.min_value)\n",
    "                \n",
    "                if row.percentile_75 is not None and row.percentile_25 is not None:\n",
    "                    numerical_stats[col]['iqr'] = float(row.percentile_75) - float(row.percentile_25)\n",
    "                \n",
    "                mean_display = f\"{row.mean_value:.2f}\" if row.mean_value is not None else \"N/A\"\n",
    "                print(f\"‚úÖ {col}: min={row.min_value}, max={row.max_value}, mean={mean_display}\")\n",
    "                \n",
    "                if is_actually_categorical:\n",
    "                    print(f\"   ‚ö†Ô∏è  {col} has only {row.distinct_count} distinct values - consider treating as categorical\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing {col}: {e}\")\n",
    "            numerical_stats[col] = {'error': str(e)}\n",
    "    \n",
    "    return numerical_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd8b6e1",
   "metadata": {},
   "source": [
    "# Step 7: Create Complete Schema Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0e01811c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema_summary(client: bigquery.Client, dataset_id: str, table_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Create a comprehensive schema summary combining all extracted information\n",
    "    \"\"\"\n",
    "    print(f\"üîç Analyzing table: {dataset_id}.{table_id}\")\n",
    "    \n",
    "    # Get basic schema\n",
    "    schema_df = get_table_schema(client, dataset_id, table_id)\n",
    "    if schema_df.empty:\n",
    "        return {}\n",
    "    \n",
    "    # Identify primary keys\n",
    "    print(\"\\nüîë Identifying Primary Keys...\")\n",
    "    primary_keys = identify_primary_keys(client, dataset_id, table_id, schema_df)\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    print(\"\\nüìä Identifying Categorical Columns...\")\n",
    "    categorical_cols = identify_categorical_columns(schema_df)\n",
    "    \n",
    "    # Get distinct values for categorical columns\n",
    "    print(\"\\nüìã Extracting Distinct Values...\")\n",
    "    distinct_values = get_categorical_distinct_values(\n",
    "        client, dataset_id, table_id, categorical_cols\n",
    "    )\n",
    "    # Identify numerical columns\n",
    "    print(\"\\nüî¢ Identifying Numerical Columns...\")\n",
    "    numerical_cols = identify_numerical_columns(schema_df)\n",
    "    \n",
    "    # Get numerical statistics\n",
    "    print(\"\\nüìà Analyzing Numerical Columns...\")\n",
    "    numerical_stats = get_numerical_column_statistics(\n",
    "        client, dataset_id, table_id, numerical_cols\n",
    "    )\n",
    "    \n",
    "    # Create summary\n",
    "    summary = {\n",
    "        'table_info': {\n",
    "            'dataset_id': dataset_id,\n",
    "            'table_id': table_id,\n",
    "            'total_columns': len(schema_df)\n",
    "        },\n",
    "        'schema': schema_df.to_dict('records'),\n",
    "        'primary_keys': primary_keys,\n",
    "        'categorical_columns': categorical_cols,\n",
    "        'numerical_columns': numerical_cols,  # ADD THIS\n",
    "        'distinct_values': distinct_values,\n",
    "        'numerical_statistics': numerical_stats,  # ADD THIS\n",
    "        'column_types': {\n",
    "            'string_columns': schema_df[schema_df['data_type'].isin(['STRING', 'BYTES'])]['column_name'].tolist(),\n",
    "            'numeric_columns': schema_df[schema_df['data_type'].isin(['INTEGER', 'INT64', 'FLOAT', 'FLOAT64', 'NUMERIC'])]['column_name'].tolist(),\n",
    "            'datetime_columns': schema_df[schema_df['data_type'].isin(['TIMESTAMP', 'DATETIME', 'DATE', 'TIME'])]['column_name'].tolist(),\n",
    "            'boolean_columns': schema_df[schema_df['data_type'] == 'BOOLEAN']['column_name'].tolist()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca161a",
   "metadata": {},
   "source": [
    "# Step 8: Run Complete Analysis for Your Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4beb90",
   "metadata": {},
   "source": [
    "## Display Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3548d0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(summary: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Display the schema summary in a readable format\n",
    "    \"\"\"\n",
    "    if not summary:\n",
    "        print(\"‚ùå No summary to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä SCHEMA ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Table info\n",
    "    table_info = summary['table_info']\n",
    "    print(f\"\\nüè∑Ô∏è  Table: {table_info['dataset_id']}.{table_info['table_id']}\")\n",
    "    print(f\"üìä Total Columns: {table_info['total_columns']}\")\n",
    "    \n",
    "    # Primary keys\n",
    "    print(f\"\\nüîë Primary Keys: {summary['primary_keys']}\")\n",
    "    \n",
    "    # Column types breakdown\n",
    "    col_types = summary['column_types']\n",
    "    print(f\"\\nüìà Column Type Breakdown:\")\n",
    "    for col_type, cols in col_types.items():\n",
    "        if cols:\n",
    "            print(f\"   {col_type}: {len(cols)} columns\")\n",
    "    \n",
    "    # Categorical columns with distinct values\n",
    "    print(f\"\\nüìã Categorical Columns ({len(summary['categorical_columns'])}):\")\n",
    "    for col in summary['categorical_columns']:\n",
    "        if col in summary['distinct_values']:\n",
    "            distinct_info = summary['distinct_values'][col]\n",
    "            if 'values' in distinct_info:\n",
    "                print(f\"   {col}: {distinct_info['total_distinct']} distinct values\")\n",
    "                # Show top 5 values\n",
    "                if distinct_info['value_counts']:\n",
    "                    top_values = distinct_info['value_counts'][:5]\n",
    "                    print(f\"      Top values: {[item['value'] for item in top_values]}\")\n",
    "\n",
    "    # Numerical columns summary\n",
    "    if 'numerical_statistics' in summary and summary['numerical_statistics']:\n",
    "        print(f\"\\nüî¢ Numerical Columns ({len(summary['numerical_columns'])}):\")\n",
    "        for col, stats in summary['numerical_statistics'].items():\n",
    "            if 'error' not in stats:\n",
    "                print(f\"   {col}:\")\n",
    "                print(f\"      Range: {stats.get('min_value', 'N/A')} to {stats.get('max_value', 'N/A')}\")\n",
    "                print(f\"      Mean: {stats.get('mean_value', 'N/A')}\")\n",
    "                print(f\"      Distinct Values: {stats.get('distinct_count', 'N/A')}\")\n",
    "                if stats.get('is_actually_categorical'):\n",
    "                    print(f\"      ‚ö†Ô∏è  Low distinct count - consider as categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d6ea4887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Complete Schema Analysis for your Sales Data table...\n",
      "üîç Analyzing table: BI_Sales_Chatbot.CUSTOMER\n",
      "‚úÖ Schema extracted for BI_Sales_Chatbot.CUSTOMER\n",
      "üìä Found 15 columns\n",
      "\n",
      "üîë Identifying Primary Keys...\n",
      "\n",
      "üìä Identifying Categorical Columns...\n",
      "üìä Identified 13 potential categorical columns\n",
      "\n",
      "üìã Extracting Distinct Values...\n",
      "‚úÖ CUSTOMER_CODE: 100000 distinct values extracted\n",
      "‚úÖ CUSTOMER_NAME: 100000 distinct values extracted\n",
      "‚úÖ COUNTRY_CODE: 150 distinct values extracted\n",
      "‚úÖ COUNTRY_NAME: 150 distinct values extracted\n",
      "‚úÖ STATE_CODE: 409 distinct values extracted\n",
      "‚úÖ STATE_NAME: 40 distinct values extracted\n",
      "‚úÖ STATE_ZONE: 6 distinct values extracted\n",
      "‚úÖ CUSTOMER_GROUP: 82500 distinct values extracted\n",
      "‚úÖ CUSTOMER_BUYING_GROUP: 29654 distinct values extracted\n",
      "‚úÖ PUBLIC_OR_PRIVATE: 1 distinct values extracted\n",
      "‚úÖ CREATED_BY: 1 distinct values extracted\n",
      "‚úÖ UPDATED_BY: 1 distinct values extracted\n",
      "‚úÖ SEC_SALES_DIST_ID: 37 distinct values extracted\n",
      "\n",
      "üî¢ Identifying Numerical Columns...\n",
      "üìä Identified 0 numerical columns\n",
      "\n",
      "üìà Analyzing Numerical Columns...\n",
      "============================================================\n",
      "üìä SCHEMA ANALYSIS SUMMARY\n",
      "============================================================\n",
      "\n",
      "üè∑Ô∏è  Table: BI_Sales_Chatbot.CUSTOMER\n",
      "üìä Total Columns: 15\n",
      "\n",
      "üîë Primary Keys: []\n",
      "\n",
      "üìà Column Type Breakdown:\n",
      "   string_columns: 13 columns\n",
      "   datetime_columns: 2 columns\n",
      "\n",
      "üìã Categorical Columns (13):\n",
      "   CUSTOMER_CODE: 100000 distinct values\n",
      "      Top values: ['889629', '509900', '908093', '891553', '895908']\n",
      "   CUSTOMER_NAME: 100000 distinct values\n",
      "      Top values: ['WALGREEN CO.', 'WALGREEN CO', 'WALGREEN EASTERN CO., INC.', \"ALBERTSON'S PHARMACY\", 'THRIFTY PAYLESS']\n",
      "   COUNTRY_CODE: 150 distinct values\n",
      "      Top values: ['IN', 'US', 'DE', 'IT', 'FR']\n",
      "   COUNTRY_NAME: 150 distinct values\n",
      "      Top values: ['India', 'USA', 'Germany', 'Italy', 'France']\n",
      "   STATE_CODE: 409 distinct values\n",
      "      Top values: ['13', '24', '25', '01', '22']\n",
      "   STATE_NAME: 40 distinct values\n",
      "      Top values: ['Maharashtra', 'Uttar Pradesh', 'West Bengal', 'Andhra Pradesh', 'Tamil Nadu']\n",
      "   STATE_ZONE: 6 distinct values\n",
      "      Top values: ['South', 'North', 'West', 'East', 'Nepal region']\n",
      "   CUSTOMER_GROUP: 82500 distinct values\n",
      "      Top values: ['', 'Others', '.', 'STANDALONE', 'CP']\n",
      "   CUSTOMER_BUYING_GROUP: 29654 distinct values\n",
      "      Top values: ['STANDALONE', 'REGIONAL ACCOUNTS', 'NATIONAL ACCOUNTS', 'PSU', 'ESI']\n",
      "   PUBLIC_OR_PRIVATE: 1 distinct values\n",
      "      Top values: ['HCM001']\n",
      "   CREATED_BY: 1 distinct values\n",
      "      Top values: ['MDM_TEAM']\n",
      "   UPDATED_BY: 1 distinct values\n",
      "      Top values: ['MDM_TEAM']\n",
      "   SEC_SALES_DIST_ID: 37 distinct values\n",
      "      Top values: ['37', '18', '76', '56', '35']\n"
     ]
    }
   ],
   "source": [
    "print(\"üöÄ Starting Complete Schema Analysis for your Sales Data table...\")\n",
    "schema_summary = create_schema_summary(client, DATASET_ID, TABLE_ID)\n",
    "\n",
    "# Display the results\n",
    "if schema_summary:\n",
    "    display_summary(schema_summary)\n",
    "else:\n",
    "    print(\"‚ùå Failed to analyze schema. Please check your BigQuery credentials and table access.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066725bf",
   "metadata": {},
   "source": [
    "# Step9- Saving the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "62a81d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_schema_for_synthetic_data(summary: Dict[str, Any], output_file: str = \"schema_for_synthetic_data.json\"):\n",
    "    \"\"\"\n",
    "    Export schema in a format optimized for synthetic data generation\n",
    "    \"\"\"\n",
    "    if not summary:\n",
    "        print(\"‚ùå No summary to export\")\n",
    "        return\n",
    "    \n",
    "    # Create synthetic data friendly format\n",
    "    synthetic_schema = {\n",
    "        'table_name': f\"{summary['table_info']['dataset_id']}.{summary['table_info']['table_id']}\",\n",
    "        'columns': {}\n",
    "    }\n",
    "    \n",
    "    for col_info in summary['schema']:\n",
    "        col_name = col_info['column_name']\n",
    "        \n",
    "        # Base column info\n",
    "        column_config = {\n",
    "            'data_type': col_info['data_type'],\n",
    "            'nullable': col_info['is_nullable'],\n",
    "            'is_primary_key': col_name in summary['primary_keys']\n",
    "        }\n",
    "        \n",
    "        # Add categorical values if available\n",
    "        if col_name in summary['distinct_values']:\n",
    "            distinct_info = summary['distinct_values'][col_name]\n",
    "            if 'values' in distinct_info:\n",
    "                column_config['categorical_values'] = distinct_info['values']\n",
    "                column_config['value_distribution'] = {\n",
    "                    item['value']: item['frequency'] \n",
    "                    for item in distinct_info['value_counts']\n",
    "                }\n",
    "        \n",
    "        synthetic_schema['columns'][col_name] = column_config\n",
    "\n",
    "        # Add numerical statistics if available\n",
    "        if col_name in summary.get('numerical_statistics', {}):\n",
    "            num_stats = summary['numerical_statistics'][col_name]\n",
    "            if 'error' not in num_stats:\n",
    "                column_config['numerical_stats'] = {\n",
    "                    'min_value': num_stats.get('min_value'),\n",
    "                    'max_value': num_stats.get('max_value'),\n",
    "                    'mean_value': num_stats.get('mean_value'),\n",
    "                    'std_dev': num_stats.get('std_dev'),\n",
    "                    'median': num_stats.get('median'),\n",
    "                    'distinct_count': num_stats.get('distinct_count'),\n",
    "                    'null_percentage': num_stats.get('null_percentage'),\n",
    "                    'is_actually_categorical': num_stats.get('is_actually_categorical', False)\n",
    "                }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(synthetic_schema, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"‚úÖ Schema exported to {output_file}\")\n",
    "    return synthetic_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "be965f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì§ Exporting schema for synthetic data generation...\n",
      "‚úÖ Schema exported to sales_data_schema_for_synthetic_CUSTOMER.json\n",
      "\n",
      "üìä Exported Schema Preview:\n",
      "Table: BI_Sales_Chatbot.CUSTOMER\n",
      "Total Columns: 15\n",
      "\n",
      "üìã CUSTOMER_CODE:\n",
      "   Type: STRING\n",
      "   Nullable: True\n",
      "   Primary Key: False\n",
      "   Sample Values: ['889629', '509900', '908093', '891553', '895908']\n",
      "   ... and 99995 more values\n",
      "\n",
      "üìã CUSTOMER_NAME:\n",
      "   Type: STRING\n",
      "   Nullable: True\n",
      "   Primary Key: False\n",
      "   Sample Values: ['WALGREEN CO.', 'WALGREEN CO', 'WALGREEN EASTERN CO., INC.', \"ALBERTSON'S PHARMACY\", 'THRIFTY PAYLESS']\n",
      "   ... and 99995 more values\n",
      "\n",
      "üìã COUNTRY_CODE:\n",
      "   Type: STRING\n",
      "   Nullable: True\n",
      "   Primary Key: False\n",
      "   Sample Values: ['IN', 'US', 'DE', 'IT', 'FR']\n",
      "   ... and 145 more values\n",
      "\n",
      "üéâ Schema extraction pipeline configured for your Sales Data table!\n",
      "üìù Ready to run - your table details are already set:\n",
      "   üìä Project: finance-analytics-291816\n",
      "   üìÇ Dataset: BI_Sales_Chatbot\n",
      "   üìã Table: CUSTOMER\n",
      "\n",
      "‚ñ∂Ô∏è  Now you can run each step to extract and validate your schema!\n",
      "üöÄ The analysis will automatically run for your specific table.\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Export Schema for Synthetic Data Generation\n",
    "print(\"\\nüì§ Exporting schema for synthetic data generation...\")\n",
    "synthetic_schema = export_schema_for_synthetic_data(\n",
    "    schema_summary, \n",
    "    output_file=f\"sales_data_schema_for_synthetic_{TABLE_ID}.json\"\n",
    ")\n",
    "\n",
    "# Show a preview of what was exported\n",
    "if synthetic_schema:\n",
    "    print(f\"\\nüìä Exported Schema Preview:\")\n",
    "    print(f\"Table: {synthetic_schema['table_name']}\")\n",
    "    print(f\"Total Columns: {len(synthetic_schema['columns'])}\")\n",
    "    \n",
    "    # Show first few columns as example\n",
    "    sample_columns = list(synthetic_schema['columns'].items())[:3]\n",
    "    for col_name, col_config in sample_columns:\n",
    "        print(f\"\\nüìã {col_name}:\")\n",
    "        print(f\"   Type: {col_config['data_type']}\")\n",
    "        print(f\"   Nullable: {col_config['nullable']}\")\n",
    "        print(f\"   Primary Key: {col_config['is_primary_key']}\")\n",
    "        if 'categorical_values' in col_config:\n",
    "            values_preview = col_config['categorical_values'][:5]\n",
    "            print(f\"   Sample Values: {values_preview}\")\n",
    "            if len(col_config['categorical_values']) > 5:\n",
    "                print(f\"   ... and {len(col_config['categorical_values']) - 5} more values\")\n",
    "\n",
    "print(\"\\nüéâ Schema extraction pipeline configured for your Sales Data table!\")\n",
    "print(\"üìù Ready to run - your table details are already set:\")\n",
    "print(f\"   üìä Project: {PROJECT_ID}\")  \n",
    "print(f\"   üìÇ Dataset: {DATASET_ID}\")\n",
    "print(f\"   üìã Table: {TABLE_ID}\")\n",
    "print(\"\\n‚ñ∂Ô∏è  Now you can run each step to extract and validate your schema!\")\n",
    "print(\"üöÄ The analysis will automatically run for your specific table.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
